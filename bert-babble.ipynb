{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=28996, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-large-cased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([MASK])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generation modes as functions '''\n",
    "\n",
    "def generate(out, gen_idx, temperature=None, top_k=0, sample=False):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = new_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1).tolist()\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1).tolist()\n",
    "    return idx\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "    \n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        kk = numpy.random.randint(0, max_len)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = mask_id\n",
    "        out = model(torch.tensor(batch))\n",
    "        idxs = generate(out, gen_idx=seed_len+kk, top_k=top_k, sample=(ii < burnin))\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and numpy.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        out = model(torch.tensor(batch))\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate(out, gen_idx=seed_len+kk, top_k=top_k, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and numpy.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "        out = model(torch.tensor(inp))\n",
    "        idxs = generate(out, gen_idx=seed_len+ii, top_k=top_k, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+ii] = idxs[jj]\n",
    "        \n",
    "    return untokenize_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 [CLS] [MASK] [MASK] pressing [MASK] , peaceful (*) and [MASK] [MASK] [MASK] reconciliation [MASK] [MASK] [MASK] in [MASK] and [MASK] pressure [MASK] [SEP]\n",
      "iter 20 [CLS] The [MASK] of reconciliation with peaceful political [MASK] [MASK] [MASK] only (*) with [MASK] [MASK] , [MASK] and political pressure . [SEP]\n",
      "iter 30 [CLS] The [MASK] promised balanced and peaceful political dialogue only (*) [MASK] only with [MASK] military , [MASK] and NGO representatives . [SEP]\n",
      "iter 40 [CLS] The [MASK] promised transparency and peaceful political dialogue only in (*) dealing with [MASK] party , political and NGO representatives . [SEP]\n",
      "iter 50 [CLS] The declaration promised transparency and limited political dialogue , in consultation (*) with [MASK] party , political and NGO leaders . [SEP]\n",
      "iter 60 [CLS] The declaration promised transparency and free (*) political dialogue , in dialogue with [MASK] religious , political and societal views . [SEP]\n",
      "iter 70 [CLS] The declaration promised transparency and (*) free political dialogue , in coordination with [MASK] religious , cultural and societal organizations . [SEP]\n",
      "iter 80 [CLS] The declaration guarantees transparency and free political dialogue , including coordination with different religious , minority and ethnic (*) organizations . [SEP]\n",
      "iter 90 [CLS] The declaration affirmed transparency and encouraged (*) political dialogue , including coordination between different religious , minority and business organizations . [SEP]\n",
      "iter 100 [CLS] The party (*) promoted tolerance and encouraged political dialogue , including coordination between the religious , minority and national organizations . [SEP]\n",
      "iter 110 [CLS] The party promoted tolerance and active political cooperation (*) , including coordination between the majority and minority political party organizations . [SEP]\n",
      "iter 120 [CLS] The party promoted close and active political dialogue (*) , including discussions between the majority and minority political party lists . [SEP]\n",
      "iter 130 [CLS] The party promoted free and active political dialogue , including communication (*) with the voters and minorities on party lists . [SEP]\n",
      "iter 140 [CLS] The party promoted free and active political dialogue ##s (*) and communication with the voters of minorities on party matters . [SEP]\n",
      "iter 150 [CLS] The party promoted open and active political dialogue (*) ##s including conversations with the voters of minorities on party level . [SEP]\n",
      "iter 160 [CLS] The party (*) promoted open and active political dialogue , including consultation with the voters of minorities on every issue . [SEP]\n",
      "iter 170 [CLS] The program promoted full and active (*) political participation , including standing with the voters of minorities on every level . [SEP]\n",
      "iter 180 [CLS] The program encourages (*) full and active civic participation , including standing by the voters of minorities in every topic . [SEP]\n",
      "iter 190 [CLS] The Forum encourages broad and active political (*) participation , including standing by the voters of Louisiana on every topic . [SEP]\n",
      "iter 200 [CLS] The party (*) supports broad and active political participation , including standing by the voters of Louisiana on every issue . [SEP]\n",
      "iter 210 [CLS] The party supports citizenship and universal (*) political participation , including participation by the voters of Serbia on every issue . [SEP]\n",
      "iter 220 [CLS] The party supports citizenship and European citizenship values , including participation for the Republic (*) of Serbia on every site . [SEP]\n",
      "iter 230 [CLS] The party supports European and Serbian citizenship ##s , including participation via (*) the Republic of Serbia on the ballot . [SEP]\n",
      "iter 240 [CLS] The party supports European and Serbian citizenship ##s , including independence of the Republic of Serbia on (*) the ballot . [SEP]\n",
      "iter 250 [CLS] The party supports European and Serbian integration ##s , including independence of the Republic of Serbia on (*) the ballot . [SEP]\n",
      "iter 260 [CLS] The party supports European and Serbian integration ##s , including listing (*) of the Republic of Serbia on the ballot . [SEP]\n",
      "iter 270 [CLS] The party supports democracy (*) and national integration ##s , including listing of the Republic of Serbia on the ballot . [SEP]\n",
      "iter 280 [CLS] The party supports autonomy and European (*) integration policies , including listing of the country as Serbia on the ballot . [SEP]\n",
      "iter 290 [CLS] The party supports European and (*) European Union policies , opposing listing of the country as Serbia on the ballot . [SEP]\n",
      "iter 300 [CLS] The party supports European and European Union policies (*) , including listing of the Serbs in Serbia on the list . [SEP]\n",
      "iter 310 [CLS] The party supports European and European Union policies , including listing all (*) the Serbs in Serbia on the list . [SEP]\n",
      "iter 320 [CLS] The coalition supports Serbian and European Union policies , including (*) listing all ethnic Serbs in Serbia on the list . [SEP]\n",
      "iter 330 [CLS] The coalition supports Serbian and European Union policies , specifically listing minority ethnic groups in Serbia on its (*) list . [SEP]\n",
      "iter 340 [CLS] The coalition supports Serbian and European Union (*) policies , specifically include other ethnic groups in Serbia on the list . [SEP]\n",
      "iter 350 [CLS] The coalition supports Serbian and (*) European Union policies , but include other ethnic groups in Kosovo on the list . [SEP]\n",
      "iter 360 [CLS] The association supports NATO and European (*) Union policies , and include all Slavic groups in Kosovo on membership lists . [SEP]\n",
      "iter 370 [CLS] The party supports NATO and European (*) Union policies , and includes all Slavic communities in Kosovo on government lists . [SEP]\n",
      "iter 380 [CLS] The government (*) supports NATO and European Union policies , and includes all ethnic communities in Kosovo on government responsibilities . [SEP]\n",
      "iter 390 [CLS] The government supports NATO and (*) European Union policies , and includes all ethnic minorities in Kosovo on its responsibilities . [SEP]\n",
      "iter 400 [CLS] The association supports NATO and European Union policies , and includes all ethnic minorities in Kosovo in its leadership (*) . [SEP]\n",
      "iter 410 [CLS] The association supports NATO and European Union policies , and includes all national minorities in Kosovo in its leadership . (*) [SEP]\n",
      "iter 420 [CLS] The party supports NATO and European Union policies , and includes the Turkish (*) minorities in Kosovo in its membership . [SEP]\n",
      "iter 430 [CLS] The organisation supports NATO and European Union policies , and includes (*) the Turkish minorities in Turkey in its membership . [SEP]\n",
      "iter 440 [CLS] The organisation supports Turkish and European Union policies , and includes the Turkish Community in Switzerland in its (*) membership . [SEP]\n",
      "iter 450 [CLS] The organisation promotes (*) Turkish - European Union cooperation , and includes the Turkish Community in Switzerland in its membership . [SEP]\n",
      "iter 460 [CLS] The organisation promotes Turkish - European Union cooperation , and includes the Turkish (*) Community in Switzerland in its membership . [SEP]\n",
      "iter 470 [CLS] The center promotes Turkey (*) - European Union Relations , and supports the Turkish Community in Brussels in its programmes . [SEP]\n",
      "iter 480 [CLS] The center promotes Turkish - European Cultural (*) Relations , and supports the Turkish Embassy in Brussels with its activities . [SEP]\n",
      "iter 490 [CLS] The center promotes Turkish - European Economic Relations , and supports the Turkish Embassy in Berlin with its projects . (*) [SEP]\n",
      "iter 500 [CLS] The center supports Turkish - European Economic Cooperation , and supports the (*) Turkish Embassy in Berlin with its building . [SEP]\n",
      "***** FINAL SENTENCES *****\n",
      "[CLS] The center supports Turkish - European Economic Cooperation , and supports the Turkish Embassy in Berlin with its building . [SEP]\n",
      "[CLS] It also is regarded as a national park for its wetland habitats , and for its domestic ##ated animals . [SEP]\n",
      "[CLS] In ##fra ##red Active Communications engineer . Information Systems engineer . Control Systems engineer . Advanced Management Systems engineer . [SEP]\n",
      "[CLS] e . g . this protein contains a similar protein domain that is identical to the actual fusion protein . [SEP]\n",
      "[CLS] English Urdu Kannada Tamil Telugu Nepali Malayalam Mongolian Students seek university entrance degree at the major Kerala government colleges . [SEP]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "max_len = 20\n",
    "top_k = 100\n",
    "temperature=1.\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 200\n",
    "sample = True\n",
    "max_iter = 500\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"[CLS]\".split()\n",
    "\n",
    "batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter)\n",
    "#batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
    "#batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
    "\n",
    "print(\"***** FINAL SENTENCES *****\")\n",
    "for sent in batch:\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence 25 in 60.228s\n",
      "Generated sentence 50 in 57.656s\n",
      "Generated sentence 75 in 57.172s\n",
      "Generated sentence 100 in 57.339s\n",
      "Generated sentence 125 in 58.277s\n",
      "Generated sentence 150 in 58.121s\n",
      "Generated sentence 175 in 58.085s\n",
      "Generated sentence 200 in 57.711s\n",
      "Generated sentence 225 in 57.998s\n",
      "Generated sentence 250 in 57.194s\n",
      "Generated sentence 275 in 57.329s\n",
      "Generated sentence 300 in 58.291s\n",
      "Generated sentence 325 in 57.264s\n",
      "Generated sentence 350 in 57.242s\n",
      "Generated sentence 375 in 57.198s\n",
      "Generated sentence 400 in 57.487s\n",
      "Generated sentence 425 in 62.306s\n",
      "Generated sentence 450 in 57.114s\n",
      "Generated sentence 475 in 57.273s\n",
      "Generated sentence 500 in 57.203s\n",
      "Generated 500 sentences in 482.908m (~57.949s/sentence)\n",
      "Generated sentence 25 in 58.081s\n",
      "Generated sentence 50 in 58.023s\n",
      "Generated sentence 75 in 57.322s\n",
      "Generated sentence 100 in 58.552s\n",
      "Generated sentence 125 in 58.004s\n",
      "Generated sentence 150 in 57.925s\n",
      "Generated sentence 175 in 57.623s\n",
      "Generated sentence 200 in 58.549s\n",
      "Generated sentence 225 in 57.307s\n",
      "Generated sentence 250 in 57.277s\n",
      "Generated sentence 275 in 58.140s\n",
      "Generated sentence 300 in 57.301s\n",
      "Generated sentence 325 in 57.392s\n",
      "Generated sentence 350 in 58.511s\n",
      "Generated sentence 375 in 57.165s\n",
      "Generated sentence 400 in 57.304s\n",
      "Generated sentence 425 in 57.957s\n",
      "Generated sentence 450 in 57.842s\n",
      "Generated sentence 475 in 57.512s\n",
      "Generated sentence 500 in 65.998s\n",
      "Generated 500 sentences in 484.398m (~58.128s/sentence)\n",
      "Generated sentence 25 in 63.436s\n",
      "Generated sentence 50 in 72.187s\n",
      "Generated sentence 75 in 75.261s\n",
      "Generated sentence 100 in 70.779s\n",
      "Generated sentence 125 in 64.371s\n",
      "Generated sentence 150 in 74.569s\n",
      "Generated sentence 175 in 57.940s\n",
      "Generated sentence 200 in 71.494s\n",
      "Generated sentence 225 in 59.047s\n",
      "Generated sentence 250 in 64.907s\n",
      "Generated sentence 275 in 72.105s\n",
      "Generated sentence 300 in 68.935s\n",
      "Generated sentence 325 in 71.085s\n",
      "Generated sentence 350 in 78.970s\n",
      "Generated sentence 375 in 66.321s\n",
      "Generated sentence 400 in 66.812s\n",
      "Generated sentence 425 in 61.138s\n",
      "Generated sentence 450 in 61.238s\n",
      "Generated sentence 475 in 61.369s\n",
      "Generated sentence 500 in 60.276s\n",
      "Generated 500 sentences in 578.444m (~69.413s/sentence)\n",
      "Generated sentence 25 in 62.137s\n",
      "Generated sentence 50 in 65.650s\n",
      "Generated sentence 75 in 67.994s\n",
      "Generated sentence 100 in 61.786s\n",
      "Generated sentence 125 in 60.044s\n",
      "Generated sentence 150 in 60.579s\n",
      "Generated sentence 175 in 61.495s\n",
      "Generated sentence 200 in 63.405s\n",
      "Generated sentence 225 in 62.274s\n",
      "Generated sentence 250 in 62.260s\n",
      "Generated sentence 275 in 62.841s\n",
      "Generated sentence 300 in 59.860s\n",
      "Generated sentence 325 in 65.125s\n",
      "Generated sentence 350 in 60.194s\n",
      "Generated sentence 375 in 61.357s\n",
      "Generated sentence 400 in 66.472s\n",
      "Generated sentence 425 in 61.892s\n",
      "Generated sentence 450 in 62.492s\n",
      "Generated sentence 475 in 63.816s\n",
      "Generated sentence 500 in 59.070s\n",
      "Generated 500 sentences in 522.725m (~62.727s/sentence)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Get some generations \"\"\"\n",
    "import time\n",
    "\n",
    "n_sample = 500\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "temperature = 1.\n",
    "burnin = 200\n",
    "max_iter = 400\n",
    "print_every = 25\n",
    "\n",
    "for top_k in [100]:\n",
    "    for temp in [.1, .5, .7, 2.]:\n",
    "        if top_k:\n",
    "            out_file = \"generations-len%d-topk%d-temp%.3f.txt\" % (max_len, top_k, temp)\n",
    "        else:\n",
    "            out_file = \"generations-len%d-burnin%d-temp%.3f.txt\" % (max_len, burnin, temp)\n",
    "\n",
    "        times = []\n",
    "        with open(out_file, \"w\") as out_fh:\n",
    "            start_time = time.time()\n",
    "            for step_n in range(n_sample):\n",
    "                seed_text = \"[CLS]\".split()\n",
    "                sent = parallel_sequential_generation(seed_text, max_len=max_len, \n",
    "                                                      top_k=top_k, temperature=temp, \n",
    "                                                      burnin=burnin, max_iter=max_iter,\n",
    "                                                      verbose=False)\n",
    "                out_fh.write(\"%s\\n\" % \" \".join(sent[1:-1]))\n",
    "                times.append(time.time() - start_time)\n",
    "                start_time = time.time()\n",
    "                if (step_n + 1) % print_every == 0:\n",
    "                    print(\"Generated sentence %d in %.3fs\" % (step_n + 1, times[-1]))\n",
    "\n",
    "        print(\"Generated %d sentences in %.3fm (~%.3fs/sentence)\" % (n_sample, sum(times) / 60, sum(times) / len(times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity measure\n",
    "from nltk.translate import bleu_score as bleu\n",
    "\n",
    "# Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference\n",
    "# lower is better?\n",
    "def self_bleu(sents):\n",
    "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
    "\n",
    "def count_ngrams(max_n=4):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality measure via outside language models\n",
    "\n",
    "# KN5 (KenLM)\n",
    "# pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "\n",
    "# Gated Convolutional LM (Fairseq)\n",
    "# https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man of wordly wealth , Sansom was primarily a business man but was also a politician .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STR = \"A man of wordly wealth, Sansom was primarily a business man but was also a politician.\"\n",
    "\" \".join(detokenize(tokenizer.tokenize(STR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scratch ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [MASK] york is the greatest city in the world . [SEP] => new ||| rank of new 1\n",
      "[CLS] new [MASK] is the greatest city in the world . [SEP] => york ||| rank of york 1\n",
      "[CLS] new york [MASK] the greatest city in the world . [SEP] => is ||| rank of is 1\n",
      "[CLS] new york is [MASK] greatest city in the world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the [MASK] city in the world . [SEP] => largest ||| rank of greatest 15\n",
      "[CLS] new york is the greatest [MASK] in the world . [SEP] => city ||| rank of city 1\n",
      "[CLS] new york is the greatest city [MASK] the world . [SEP] => in ||| rank of in 1\n",
      "[CLS] new york is the greatest city in [MASK] world . [SEP] => the ||| rank of the 1\n",
      "[CLS] new york is the greatest city in the [MASK] . [SEP] => world ||| rank of world 1\n",
      "[CLS] new york is the greatest city in the world [MASK] [SEP] => . ||| rank of . 1\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "original_sent = [CLS] + 'new york is the greatest city in the world . '.lower().split() + [SEP]\n",
    "\n",
    "for ii_ in range(len(original_sent)-2):\n",
    "    ii = ii_ + 1\n",
    "    new_sent = copy.copy(original_sent)\n",
    "    new_sent[ii] = '[MASK]'\n",
    "#     new_sent[ii] = tokenizer.convert_ids_to_tokens([numpy.random.randint(0, len(tokenizer.vocab))])[0]\n",
    "    out = model(torch.tensor([tokenizer.convert_tokens_to_ids(new_sent)]))\n",
    "    pred = tokenizer.convert_ids_to_tokens([out[0][ii].max(0)[1].item()])[0]\n",
    "    probs = out[0][ii].data.numpy()\n",
    "    rank = len(tokenizer.vocab) - numpy.argsort(numpy.argsort(probs))[tokenizer.convert_tokens_to_ids([original_sent[ii]])[0]]\n",
    "    print(\" \".join(new_sent), \"=>\", pred, '|||', 'rank of', original_sent[ii], rank)\n",
    "#     if pred == 'the':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" . . . . . , . . , . . . . . , . . . . [MASK]\n",
      "\" and Felix - ( - ) = + . - = . he = . - = . . [MASK]\n",
      "the and formula ##s ; , and - algebra ; ; . . . . . ; . . . [MASK]\n",
      "/ was . by . . . . . . and , from , gave . . . . . [MASK]\n",
      "* by army use of and as the of were applied as ( ( ) , and = ) . [MASK]\n",
      ". and ##i . . , . . . and ... , . . , to the part , . [MASK]\n",
      ". you ; ; ; ; ; ' Mr . Scott to the and ##ra of the . \" . [MASK]\n",
      ". . ##1 . : and . . . : . . : . . . . . . . [MASK]\n",
      "king - as of 2014 . | . / _ . / < < - | . / > | [MASK]\n",
      ". ##2 = . . = = = = - . = = = = ( = = ) | [MASK]\n"
     ]
    }
   ],
   "source": [
    "''' sequential generation: this one kinda works '''\n",
    "\n",
    "\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])\n",
    "sample = True\n",
    "max_len = 20\n",
    "leed_out_len = 5 #max_len\n",
    "random_future = False\n",
    "top_k = 100 # set it to 0 if you don't want top_k\n",
    "n_samples = 1\n",
    "\n",
    "seed_text = [[CLS] for _ in range(batch_size)]\n",
    "seed_len = len(seed_text[0])\n",
    "\n",
    "for si in range(n_samples):\n",
    "    #init_text = seed_text + ['[MASK]'] * max_len\n",
    "    init_text = [seed + ['[MASK]'] * max_len for seed in seed_text]\n",
    "    init_idx = tokenize_batch(init_text) #tokenizer.convert_tokens_to_ids(init_text)\n",
    "    #if random_future:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        out = model(torch.tensor([i[:seed_len+ii+leed_out_len]+sep_id for i in init_idx]))\n",
    "        if top_k > 0:\n",
    "            logits = out[:,seed_len+ii]\n",
    "            kth_vals, kth_idx = logits.topk(top_k, dim=1)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            new_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
    "            for jj in range(len(init_idx)):\n",
    "                init_idx[jj][ii] = new_idxs[jj]\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+ii])\n",
    "                init_idx[seed_len+ii] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+ii] = torch.max(out[0, seed_len+ii],0)[1].item()\n",
    "\n",
    "#     print(init_idx)\n",
    "    for sent in init_idx:\n",
    "        print(\" \".join(tokenizer.convert_ids_to_tokens(sent)))\n",
    "# print(\" \".join(tokenizer.convert_ids_to_tokens(init_idx)).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[119], [119], [1103], [170], [168], [1110], [119], [176], [119], [119]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 [CLS] philippine \" ##hara ##id on mir by character sons five god with the , ; for a fatal ##in ; [SEP]\n",
      "iter 11 [CLS] 2 m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 21 [CLS] 2 ##m be ##h on ##r by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 31 [CLS] 2 ##m be ##h on ##s by the to . god with . aid ; for present definite h . [SEP]\n",
      "iter 41 [CLS] 2 ##m be ##h on ze by the to . god with . aid ; for which definite h . [SEP]\n",
      "iter 51 [CLS] 2 ##m be ##h on - by the to . god with . aid ; p or an h . [SEP]\n",
      "iter 61 [CLS] 2 ##m be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 71 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 81 [CLS] 2 ##b be ##h on made by the to . god with . help the p or an h . [SEP]\n",
      "iter 91 [CLS] 2 ##b be ##h is made by the to . god with . help the p or an h . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel generation: this one doesn't work '''\n",
    "\n",
    "sample = True\n",
    "max_iter = 100\n",
    "viz_int = 10\n",
    "max_len = 20\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * max_len + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "# for ii in range(max_len):\n",
    "#     init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    for kk in range(max_len):\n",
    "        if top_k > 0:\n",
    "            logits = out[0,seed_len+kk]\n",
    "            kth_vals, kth_idx = logits.topk(top_k)\n",
    "            dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "            init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "        else:\n",
    "            if sample:\n",
    "                dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "                init_idx[seed_len+kk] = dist.sample().item()\n",
    "            else:\n",
    "                init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "    if numpy.mod(ii, viz_int) == 0:\n",
    "        print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(init_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 [CLS] un (*) ##i [MASK] [MASK] [MASK] ; [MASK] . [MASK] ##i [MASK] : ; [MASK] [MASK] [SEP]\n",
      "iter 20 [CLS] xx ##ix [MASK] ; [MASK] . [MASK] . (*) [MASK] . [MASK] 2 ; b [MASK] [SEP]\n",
      "iter 30 [CLS] vi (*) . [MASK] ; [MASK] . [MASK] . 17 .  2 : ii . [SEP]\n",
      "iter 40 [CLS] iii . 11 ; norway . iii . 17 .  87 (*) . 1 . [SEP]\n",
      "iter 50 [CLS] iii . (*) sweden . norway  11 . 17 &  87 . 12 . [SEP]\n",
      "iter 60 [CLS] iii . denmark (*) & norway  11 . 17 .  87 . 20 . [SEP]\n",
      "iter 70 [CLS] iii . denmark & norway  87 . 17 ;  87 . (*) 20 ; [SEP]\n",
      "iter 80 [CLS] 4 . denmark & norway  87 (*) . 17 ;  87 . 20 ; [SEP]\n",
      "iter 90 [CLS] 4 . denmark & sweden  (*) 85 . 11 ;  87 . 20 ; [SEP]\n",
      "iter 100 [CLS] 4 - denmark & norway (*)  86 . 6 ;  86 . 2 ; [SEP]\n",
      "iter 110 [CLS] cf . denmark - (*) norway  86 . 5 ;  86 . 7 ; [SEP]\n",
      "iter 120 [CLS] cf . denmark - schleswig (*)  1886 . 5 ,  86 . 1 ; [SEP]\n",
      "iter 130 [CLS] cf . denmark v schleswig  86 . (*) 5 ,  86 . 1 ; [SEP]\n",
      "iter 140 [CLS] cf . (*) denmark ser .  86 . 2 ,  86 . 6 ; [SEP]\n",
      "iter 150 [CLS] cf . denmark ser .  86 . 0 ,  86 . 2 ; (*) [SEP]\n",
      "iter 160 [CLS] em . denmark 56 (*) .  86 . 0 ;  86 . 45 ; [SEP]\n",
      "iter 170 [CLS] 1 (*) . denmark 56 .  86 . 42 ;  86 . 45 ; [SEP]\n",
      "iter 180 [CLS] 5 . ch 56 .  86 . 42 ,  86 (*) . 45 . [SEP]\n",
      "iter 190 [CLS] 5 . ch ##s .  86 . 35 ,  (*) 86 . 45 . [SEP]\n",
      "iter 200 [CLS] 5 . pre ##s .  86 . 35 and  86 . 45 . (*) [SEP]\n",
      "iter 210 [CLS] 5 . con ##s .  86 . 44 . (*)  86 . 45 . [SEP]\n",
      "iter 220 [CLS] 5 . con ##st .  (*) 86 . 44 .  86 . 45 . [SEP]\n",
      "iter 230 [CLS] 5 . di ##st .  (*) 86 . 44 ,  86 . 45 . [SEP]\n",
      "iter 240 [CLS] 5 . di ##st .  86 . 44 ,  (*) 86 . 45 . [SEP]\n",
      "iter 250 [CLS] stat . di ##st .  86 . 44 (*) .  86 . 45 . [SEP]\n",
      "iter 260 [CLS] stat . di ##st .  86 . 44 .  86 . 45 . (*) [SEP]\n",
      "iter 270 [CLS] stat . di ##st .  (*) 86 . 44 .  86 . 45 . [SEP]\n",
      "iter 280 [CLS] stat . di ##st .  86 (*) . 44 .  86 . 45 . [SEP]\n",
      "iter 290 [CLS] stat . di ##st .  86 . (*) 44 .  86 . 45 . [SEP]\n",
      "iter 300 [CLS] stat (*) . di ##st .  86 . 44 .  86 . 45 . [SEP]\n"
     ]
    }
   ],
   "source": [
    "''' parallel-sequential generation: this one definitely works '''\n",
    "\n",
    "# sample = True\n",
    "burnin = 200\n",
    "max_iter = 300\n",
    "viz_int = 10\n",
    "max_len = 15\n",
    "top_k = 0\n",
    "\n",
    "seed_text = '[CLS]'.split()\n",
    "seed_len = len(seed_text)\n",
    "\n",
    "init_text = seed_text + ['[MASK]'] * (max_len) + ['[SEP]']\n",
    "init_idx = tokenizer.convert_tokens_to_ids(init_text)\n",
    "#for ii in range(max_len):\n",
    "#    init_idx[seed_len+ii] = numpy.random.randint(0, len(tokenizer.vocab))\n",
    "\n",
    "for ii in range(max_iter):\n",
    "    kk = numpy.random.randint(0, max_len)\n",
    "    init_idx[seed_len+kk] = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    out = model(torch.tensor([init_idx]))\n",
    "    if top_k > 0:\n",
    "        logits = out[0,seed_len+kk]\n",
    "        kth_vals, kth_idx = logits.topk(top_k)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        init_idx[seed_len+kk] = kth_idx[dist.sample().item()].item()\n",
    "    else:\n",
    "        if ii < burnin:\n",
    "            dist = torch.distributions.categorical.Categorical(logits=out[0,seed_len+kk])\n",
    "            init_idx[seed_len+kk] = dist.sample().item()\n",
    "        else:\n",
    "            init_idx[seed_len+kk] = torch.max(out[0, seed_len+kk],0)[1].item()\n",
    "        \n",
    "    if numpy.mod(ii+1, viz_int) == 0:\n",
    "        for_print = tokenizer.convert_ids_to_tokens(init_idx)\n",
    "        for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "        print(\"iter\", ii+1, \" \".join(for_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
